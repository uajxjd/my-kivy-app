"""
FC3Dæ™ºèƒ½äº¤äº’å¼é¢„æµ‹ç³»ç»Ÿ - å¢å¼ºä¼˜åŒ–ç‰ˆ
Author: AI Assistant
Date: 2024
Description: åŸºäºå…ˆè¿›AIæ¶æ„çš„ç¦å½©3Dé¢„æµ‹ç³»ç»Ÿï¼ŒåŒ…å«å®Œæ•´çš„è¶…å‚æ•°ä¼˜åŒ–æ¨¡å—å’Œå¢å¼ºè®­ç»ƒç­–ç•¥
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import warnings
import pickle
import os
from abc import ABC, abstractmethod
from typing import Tuple, List, Dict, Any, Optional, Union
import sys
from datetime import datetime, timedelta
import time
from tqdm import tqdm
import matplotlib.pyplot as plt
import logging
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.model_selection import TimeSeriesSplit
import optuna
from optuna import Trial
from optuna.samplers import TPESampler
import json
import hashlib

warnings.filterwarnings('ignore')

# è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡å¤æ€§
torch.manual_seed(42)
np.random.seed(42)


# è®¾ç½®æ—¥å¿—
def setup_logging():
    """è®¾ç½®æ—¥å¿—è®°å½•"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('fc3d_predictor_enhanced.log', encoding='utf-8')
        ]
    )


setup_logging()


# å¸¸é‡å®šä¹‰
class EnhancedConstants:
    """å¢å¼ºç‰ˆå¸¸é‡å®šä¹‰"""
    DEFAULT_SEQUENCE_LENGTH = 60
    BATCH_SIZE = 64
    MAX_EPOCHS = 200
    PATIENCE = 20
    LEARNING_RATE = 2e-4
    VALIDATION_SPLIT = 0.15
    MIN_SEQUENCE_LENGTH = 20
    OPTIMIZATION_TRIALS = 50
    OPTIMIZATION_TIMEOUT = 3600
    MIN_DATA_POINTS = 200
    WARMUP_EPOCHS = 10
    GRADIENT_ACCUMULATION_STEPS = 4
    LABEL_SMOOTHING = 0.1
    TOP_CANDIDATES = 6  # ä¿®æ”¹ä¸º6ä¸ªå€™é€‰æ•°å­—


# è‡ªå®šä¹‰å¼‚å¸¸
class ModelValidationError(Exception):
    """æ¨¡å‹éªŒè¯é”™è¯¯"""
    pass


class DataValidationError(Exception):
    """æ•°æ®éªŒè¯é”™è¯¯"""
    pass


class OptimizationError(Exception):
    """è¶…å‚æ•°ä¼˜åŒ–é”™è¯¯"""
    pass


class ConfigurationError(Exception):
    """é…ç½®é”™è¯¯"""
    pass


class ProgressBar:
    """è‡ªå®šä¹‰è¿›åº¦æ¡ç±»"""

    def __init__(self, total: int, desc: str = "Processing"):
        self.total = total
        self.desc = desc
        self.start_time = None
        self.current = 0

    def __enter__(self):
        self.start_time = time.time()
        self.pbar = tqdm(total=self.total, desc=self.desc,
                         bar_format='{l_bar}{bar:50}{r_bar}{bar:-50b}')
        return self

    def update(self, n: int = 1, **kwargs):
        """æ›´æ–°è¿›åº¦æ¡"""
        self.current += n
        elapsed = time.time() - self.start_time
        if self.current > 0:
            avg_time_per_step = elapsed / self.current
            remaining_time = avg_time_per_step * (self.total - self.current)
            remaining_str = str(timedelta(seconds=int(remaining_time)))
        else:
            remaining_str = "Calculating..."

        self.pbar.set_postfix({
            **kwargs,
            'remaining': remaining_str
        })
        self.pbar.update(n)

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.pbar.close()


class DataQualityValidator:
    """æ•°æ®è´¨é‡éªŒè¯å™¨"""

    @staticmethod
    def validate_data_structure(data: pd.DataFrame) -> Dict[str, Any]:
        """éªŒè¯æ•°æ®ç»“æ„"""
        validation_result = {
            'is_valid': True,
            'errors': [],
            'warnings': [],
            'summary': {}
        }

        # æ£€æŸ¥å¿…éœ€åˆ—
        required_columns = ['æœŸå·', 'ç™¾ä½', 'åä½', 'ä¸ªä½', 'å’Œå€¼', 'è·¨åº¦', 'é‡å¤æ•°å­—']
        missing_columns = [col for col in required_columns if col not in data.columns]

        if missing_columns:
            validation_result['is_valid'] = False
            validation_result['errors'].append(f"ç¼ºå°‘å¿…éœ€åˆ—: {missing_columns}")

        # æ£€æŸ¥æ•°æ®ç±»å‹
        if 'ç™¾ä½' in data.columns:
            if not pd.api.types.is_numeric_dtype(data['ç™¾ä½']):
                validation_result['errors'].append("ç™¾ä½åˆ—å¿…é¡»ä¸ºæ•°å€¼ç±»å‹")

        # æ£€æŸ¥æ•°å€¼èŒƒå›´
        for col in ['ç™¾ä½', 'åä½', 'ä¸ªä½']:
            if col in data.columns:
                if data[col].min() < 0 or data[col].max() > 9:
                    validation_result['errors'].append(f"{col}æ•°å€¼è¶…å‡ºèŒƒå›´(0-9)")

        # æ£€æŸ¥ç¼ºå¤±å€¼
        missing_values = data[required_columns].isnull().sum()
        if missing_values.any():
            validation_result['warnings'].append(f"å­˜åœ¨ç¼ºå¤±å€¼: {missing_values.to_dict()}")

        # æ£€æŸ¥é‡å¤æ•°æ®
        duplicates = data.duplicated().sum()
        if duplicates > 0:
            validation_result['warnings'].append(f"å­˜åœ¨{duplicates}æ¡é‡å¤æ•°æ®")

        # ç”Ÿæˆæ‘˜è¦
        validation_result['summary'] = {
            'total_records': len(data),
            'date_range': {
                'start': data['æœŸå·'].min() if 'æœŸå·' in data.columns else 'N/A',
                'end': data['æœŸå·'].max() if 'æœŸå·' in data.columns else 'N/A'
            },
            'value_ranges': {
                col: (data[col].min(), data[col].max())
                for col in ['ç™¾ä½', 'åä½', 'ä¸ªä½']
                if col in data.columns
            }
        }

        return validation_result

    @staticmethod
    def generate_data_report(data: pd.DataFrame) -> str:
        """ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š"""
        validation = DataQualityValidator.validate_data_structure(data)

        report_lines = ["=" * 60, "æ•°æ®è´¨é‡éªŒè¯æŠ¥å‘Š", "=" * 60]

        if validation['is_valid']:
            report_lines.append("âœ… æ•°æ®ç»“æ„éªŒè¯é€šè¿‡")
        else:
            report_lines.append("âŒ æ•°æ®ç»“æ„éªŒè¯å¤±è´¥:")
            for error in validation['errors']:
                report_lines.append(f"   - {error}")

        if validation['warnings']:
            report_lines.append("\nâš ï¸ è­¦å‘Šä¿¡æ¯:")
            for warning in validation['warnings']:
                report_lines.append(f"   - {warning}")

        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        report_lines.append("\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
        for key, value in validation['summary'].items():
            if isinstance(value, dict):
                report_lines.append(f"   {key}:")
                for sub_key, sub_value in value.items():
                    report_lines.append(f"     {sub_key}: {sub_value}")
            else:
                report_lines.append(f"   {key}: {value}")

        report_lines.append("=" * 60)
        return "\n".join(report_lines)


class ConfigManager:
    """é…ç½®ç®¡ç†å™¨"""

    CONFIG_FILE = "fc3d_system_config_enhanced.json"

    @classmethod
    def save_config(cls, predictor, filepath: str = None) -> bool:
        """ä¿å­˜ç³»ç»Ÿé…ç½®"""
        try:
            if filepath is None:
                filepath = cls.CONFIG_FILE

            # å‡†å¤‡å¯åºåˆ—åŒ–çš„ä¼˜åŒ–ç»“æœ
            serializable_optimization_results = {}
            for model_type, results in predictor.optimization_results.items():
                serializable_optimization_results[model_type] = {
                    'best_params': results.get('best_params', {}),
                    'best_score': results.get('best_score', 0)
                }

            config = {
                'system_info': {
                    'version': '2.0.0',
                    'save_time': datetime.now().isoformat(),
                    'data_hash': cls._calculate_data_hash(predictor.data) if predictor.data is not None else None
                },
                'feature_columns': predictor.feature_columns,
                'current_period': predictor.current_period,
                'model_status': {
                    name: {
                        'is_trained': info.get('is_trained', False),
                        'has_optimized_params': info.get('optimized_params') is not None,
                        'best_val_acc': info.get('info', {}).get('best_val_acc', 0)
                    }
                    for name, info in predictor.models.items()
                },
                'optimization_results': serializable_optimization_results,
                'data_summary': {
                    'total_records': len(predictor.data) if predictor.data is not None else 0,
                    'feature_count': len(predictor.feature_columns) if predictor.feature_columns else 0
                }
            }

            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(config, f, indent=2, ensure_ascii=False)

            logging.info(f"ç³»ç»Ÿé…ç½®å·²ä¿å­˜: {filepath}")
            return True

        except Exception as e:
            logging.error(f"ä¿å­˜é…ç½®å¤±è´¥: {e}")
            return False

    @classmethod
    def load_config(cls, predictor, filepath: str = None) -> bool:
        """åŠ è½½ç³»ç»Ÿé…ç½®"""
        try:
            if filepath is None:
                filepath = cls.CONFIG_FILE

            if not os.path.exists(filepath):
                logging.warning(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
                return False

            with open(filepath, 'r', encoding='utf-8') as f:
                config = json.load(f)

            # éªŒè¯é…ç½®å®Œæ•´æ€§
            if not cls._validate_config(config):
                raise ConfigurationError("é…ç½®æ–‡ä»¶æ ¼å¼æ— æ•ˆ")

            # åº”ç”¨é…ç½®
            predictor.feature_columns = config.get('feature_columns', [])
            predictor.current_period = config.get('current_period')
            predictor.optimization_results = config.get('optimization_results', {})

            # æ›´æ–°æ¨¡å‹çŠ¶æ€
            model_status = config.get('model_status', {})
            for model_type, status in model_status.items():
                if model_type in predictor.models:
                    predictor.models[model_type]['is_trained'] = status.get('is_trained', False)

            logging.info(f"ç³»ç»Ÿé…ç½®å·²åŠ è½½: {filepath}")
            return True

        except Exception as e:
            logging.error(f"åŠ è½½é…ç½®å¤±è´¥: {e}")
            return False

    @staticmethod
    def _calculate_data_hash(data: pd.DataFrame) -> str:
        """è®¡ç®—æ•°æ®å“ˆå¸Œå€¼"""
        if data is None:
            return ""
        return hashlib.md5(pd.util.hash_pandas_object(data).values.tobytes()).hexdigest()

    @staticmethod
    def _validate_config(config: Dict) -> bool:
        """éªŒè¯é…ç½®æ ¼å¼"""
        required_sections = ['system_info', 'model_status']
        return all(section in config for section in required_sections)


class PerformanceTracker:
    """æ¨¡å‹æ€§èƒ½è·Ÿè¸ªå™¨"""

    def __init__(self):
        self.performance_history = {}

    def track_prediction(self, model_type: str, predicted: List, actual: List, period: str):
        """è·Ÿè¸ªé¢„æµ‹æ€§èƒ½"""
        if model_type not in self.performance_history:
            self.performance_history[model_type] = []

        # è®¡ç®—å‡†ç¡®ç‡
        bai_correct = predicted[0] == actual[0]
        shi_correct = predicted[1] == actual[1]
        ge_correct = predicted[2] == actual[2]
        all_correct = bai_correct and shi_correct and ge_correct

        performance_data = {
            'timestamp': datetime.now().isoformat(),
            'period': period,
            'predicted': predicted,
            'actual': actual,
            'accuracy': {
                'bai': bai_correct,
                'shi': shi_correct,
                'ge': ge_correct,
                'all': all_correct
            }
        }

        self.performance_history[model_type].append(performance_data)

        # é™åˆ¶å†å²è®°å½•é•¿åº¦
        if len(self.performance_history[model_type]) > 1000:
            self.performance_history[model_type] = self.performance_history[model_type][-1000:]

    def get_performance_summary(self, model_type: str) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æ‘˜è¦"""
        if model_type not in self.performance_history or not self.performance_history[model_type]:
            return {}

        history = self.performance_history[model_type]
        total_predictions = len(history)

        accuracies = {
            'bai': sum(1 for h in history if h['accuracy']['bai']) / total_predictions,
            'shi': sum(1 for h in history if h['accuracy']['shi']) / total_predictions,
            'ge': sum(1 for h in history if h['accuracy']['ge']) / total_predictions,
            'all': sum(1 for h in history if h['accuracy']['all']) / total_predictions
        }

        return {
            'total_predictions': total_predictions,
            'accuracy_rates': accuracies,
            'recent_performance': history[-10:] if len(history) >= 10 else history
        }

    def save_performance_data(self, filepath: str = "performance_history_enhanced.json"):
        """ä¿å­˜æ€§èƒ½æ•°æ®"""
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(self.performance_history, f, indent=2, ensure_ascii=False)
            logging.info(f"æ€§èƒ½æ•°æ®å·²ä¿å­˜: {filepath}")
        except Exception as e:
            logging.error(f"ä¿å­˜æ€§èƒ½æ•°æ®å¤±è´¥: {e}")

    def load_performance_data(self, filepath: str = "performance_history_enhanced.json"):
        """åŠ è½½æ€§èƒ½æ•°æ®"""
        try:
            if os.path.exists(filepath):
                with open(filepath, 'r', encoding='utf-8') as f:
                    self.performance_history = json.load(f)
                logging.info(f"æ€§èƒ½æ•°æ®å·²åŠ è½½: {filepath}")
        except Exception as e:
            logging.error(f"åŠ è½½æ€§èƒ½æ•°æ®å¤±è´¥: {e}")


class EnhancedFC3DDataset(Dataset):
    """å¢å¼ºç‰ˆç¦å½©3Dæ•°æ®é›†ç±»"""

    def __init__(self, data: pd.DataFrame, sequence_length: int = EnhancedConstants.DEFAULT_SEQUENCE_LENGTH,
                 feature_columns: Optional[List[str]] = None,
                 fit_scaler: bool = True,
                 external_scaler: Optional[StandardScaler] = None,
                 for_prediction: bool = False):
        self.data = data.reset_index(drop=True)
        self.sequence_length = sequence_length
        self.feature_columns = feature_columns
        self.scaler = external_scaler
        self.for_prediction = for_prediction
        self._prepare_features(fit_scaler)

    def _prepare_features(self, fit_scaler: bool = True):
        """å¢å¼ºç‰¹å¾å·¥ç¨‹"""
        df = self.data.copy()

        # åŸºç¡€ç‰¹å¾
        df['period'] = df.index

        # æ›´ä¸°å¯Œçš„æŠ€æœ¯æŒ‡æ ‡
        for window in [3, 5, 10, 15, 20]:
            # ç§»åŠ¨å¹³å‡
            for col in ['ç™¾ä½', 'åä½', 'ä¸ªä½']:
                df[f'{col}_ma_{window}'] = df[col].rolling(window, min_periods=1).mean()
                df[f'{col}_std_{window}'] = df[col].rolling(window, min_periods=1).std()

            # å’Œå€¼ä¸è·¨åº¦çš„æŠ€æœ¯æŒ‡æ ‡
            df[f'å’Œå€¼_ma_{window}'] = df['å’Œå€¼'].rolling(window, min_periods=1).mean()
            df[f'è·¨åº¦_ma_{window}'] = df['è·¨åº¦'].rolling(window, min_periods=1).mean()

        # å¢å¼ºæ»åç‰¹å¾
        for lag in [1, 2, 3, 5, 7, 10, 15]:
            for col in ['ç™¾ä½', 'åä½', 'ä¸ªä½']:
                df[f'{col}_lag_{lag}'] = df[col].shift(lag)

        # å‘¨æœŸç‰¹å¾
        df['day_of_week'] = df.index % 7  # å‡è®¾æ¯å¤©ä¸€æœŸ
        df['period_in_month'] = df.index % 30

        # ç»Ÿè®¡ç‰¹å¾
        for col in ['ç™¾ä½', 'åä½', 'ä¸ªä½']:
            df[f'{col}_rolling_skew_10'] = df[col].rolling(10, min_periods=1).skew()
            df[f'{col}_rolling_kurt_10'] = df[col].rolling(10, min_periods=1).kurt()

        # ç»„åˆç‰¹å¾
        df['ç™¾åç»„åˆ'] = df['ç™¾ä½'] * 10 + df['åä½']
        df['åä¸ªç»„åˆ'] = df['åä½'] * 10 + df['ä¸ªä½']
        df['ç™¾ä¸ªç»„åˆ'] = df['ç™¾ä½'] * 10 + df['ä¸ªä½']

        # çƒ­ç¼–ç é‡å¤æ•°å­—
        df['é‡å¤æ•°_0'] = (df['é‡å¤æ•°å­—'] == 0).astype(int)
        df['é‡å¤æ•°_1'] = (df['é‡å¤æ•°å­—'] == 1).astype(int)
        df['é‡å¤æ•°_2'] = (df['é‡å¤æ•°å­—'] == 2).astype(int)

        # è¶‹åŠ¿ç‰¹å¾
        for col in ['ç™¾ä½', 'åä½', 'ä¸ªä½']:
            df[f'{col}_trend_5'] = df[col].rolling(5, min_periods=1).apply(
                lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) > 1 else 0, raw=True
            )

        # æ³¢åŠ¨ç‡ç‰¹å¾
        for col in ['ç™¾ä½', 'åä½', 'ä¸ªä½']:
            df[f'{col}_volatility_10'] = df[col].rolling(10, min_periods=1).std() / df[col].rolling(10,
                                                                                                    min_periods=1).mean()

        # å¡«å……NaNå€¼ - ä½¿ç”¨æ›´æ™ºèƒ½çš„å¡«å……
        df = df.fillna(method='ffill').fillna(method='bfill').fillna(0)

        # ç¡®å®šç‰¹å¾åˆ—
        if self.feature_columns is None:
            exclude_columns = ['æœŸå·', 'ç™¾ä½', 'åä½', 'ä¸ªä½', 'é‡å¤æ•°å­—']
            self.feature_columns = [col for col in df.columns if col not in exclude_columns]

        # ä½¿ç”¨RobustScalerä»£æ›¿StandardScaler
        if fit_scaler and self.scaler is None:
            self.scaler = RobustScaler()
            feature_data = df[self.feature_columns].values
            scaled_features = self.scaler.fit_transform(feature_data)
            self.features = df.copy()
            self.features[self.feature_columns] = scaled_features
        elif self.scaler is not None:
            feature_data = df[self.feature_columns].values
            scaled_features = self.scaler.transform(feature_data)
            self.features = df.copy()
            self.features[self.feature_columns] = scaled_features
        else:
            self.features = df

    def get_feature_dimension(self) -> int:
        """è·å–ç‰¹å¾ç»´åº¦"""
        return len(self.feature_columns)

    def get_feature_columns(self) -> List[str]:
        """è·å–ç‰¹å¾åˆ—å"""
        return self.feature_columns.copy()

    def get_scaler(self) -> Optional[StandardScaler]:
        """è·å–æ ‡å‡†åŒ–å™¨"""
        return self.scaler

    def __len__(self):
        if self.for_prediction:
            return 1 if len(self.data) >= self.sequence_length else 0
        else:
            return max(0, len(self.data) - self.sequence_length)

    def __getitem__(self, idx):
        if self.for_prediction:
            if len(self.features) < self.sequence_length:
                raise DataValidationError("æ•°æ®ä¸è¶³è¿›è¡Œé¢„æµ‹")

            start_idx = len(self.features) - self.sequence_length
            sequence_data = self.features.iloc[start_idx:start_idx + self.sequence_length]

            features = sequence_data[self.feature_columns].values.astype(np.float32)

            return (
                torch.FloatTensor(features),
                torch.LongTensor([0]),
                torch.LongTensor([0]),
                torch.LongTensor([0])
            )
        else:
            if idx + self.sequence_length >= len(self.features):
                idx = len(self.features) - self.sequence_length - 1

            if idx < 0:
                idx = 0

            sequence_data = self.features.iloc[idx:idx + self.sequence_length]
            features = sequence_data[self.feature_columns].values.astype(np.float32)

            target_period = idx + self.sequence_length
            if target_period >= len(self.data):
                target_period = len(self.data) - 1

            target_bai = self.data.iloc[target_period]['ç™¾ä½']
            target_shi = self.data.iloc[target_period]['åä½']
            target_ge = self.data.iloc[target_period]['ä¸ªä½']

            return (
                torch.FloatTensor(features),
                torch.LongTensor([target_bai]),
                torch.LongTensor([target_shi]),
                torch.LongTensor([target_ge])
            )


class BaseModel(ABC, nn.Module):
    """åŸºç¡€æ¨¡å‹æŠ½è±¡ç±»"""

    def __init__(self, input_dim: int, hidden_dim: int = 256):
        super().__init__()
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim

    @abstractmethod
    def forward(self, x):
        pass

    def predict_top6(self, x):
        """é¢„æµ‹æ¯ä¸ªä½ç½®çš„å‰6ä¸ªæœ€å¯èƒ½æ•°å­—"""
        with torch.no_grad():
            outputs = self.forward(x)
            if isinstance(outputs, tuple):
                bai_probs, shi_probs, ge_probs = outputs
            else:
                bai_probs, shi_probs, ge_probs = outputs.chunk(3, dim=1)

            bai_top6 = torch.topk(bai_probs, EnhancedConstants.TOP_CANDIDATES, dim=1)[1].cpu().numpy()[0]
            shi_top6 = torch.topk(shi_probs, EnhancedConstants.TOP_CANDIDATES, dim=1)[1].cpu().numpy()[0]
            ge_top6 = torch.topk(ge_probs, EnhancedConstants.TOP_CANDIDATES, dim=1)[1].cpu().numpy()[0]

            return bai_top6, shi_top6, ge_top6


class EnhancedTemporalMoE(BaseModel):
    """å¢å¼ºç‰ˆæ—¶åºæ··åˆä¸“å®¶æ¨¡å‹"""

    def __init__(self, input_dim: int, hidden_dim: int = 256, num_experts: int = 8,
                 dropout_rate: float = 0.2, expert_dropout: float = 0.1):
        super().__init__(input_dim, hidden_dim)

        # å¢å¼ºçš„ä¸“å®¶ç½‘ç»œ
        expert_output_dim = hidden_dim // 2
        self.experts = nn.ModuleList([
            nn.Sequential(
                nn.Linear(input_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.GELU(),
                nn.Dropout(expert_dropout),
                nn.Linear(hidden_dim, expert_output_dim),
                nn.BatchNorm1d(expert_output_dim),
                nn.GELU(),
                nn.Dropout(expert_dropout // 2)
            ) for _ in range(num_experts)
        ])

        # å¢å¼ºçš„é—¨æ§ç½‘ç»œ
        self.gate = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, num_experts),
            nn.Softmax(dim=-1)
        )

        # æ®‹å·®è¿æ¥
        self.residual_linear = nn.Linear(input_dim, expert_output_dim)

        # æ—¶åºå¤„ç†å±‚
        self.temporal_processor = nn.LSTM(
            input_size=expert_output_dim,
            hidden_size=expert_output_dim,
            num_layers=2,
            batch_first=True,
            dropout=0.1,
            bidirectional=True
        )

        # LSTMè¾“å‡ºç»´åº¦è°ƒæ•´ï¼ˆåŒå‘ï¼‰
        lstm_output_dim = expert_output_dim * 2

        # æ³¨æ„åŠ›æœºåˆ¶
        self.attention = nn.MultiheadAttention(
            embed_dim=lstm_output_dim,
            num_heads=8,
            dropout=0.1,
            batch_first=True
        )

        # å±‚å½’ä¸€åŒ–
        self.layer_norm1 = nn.LayerNorm(lstm_output_dim)
        self.layer_norm2 = nn.LayerNorm(lstm_output_dim)

        # å¢å¼ºçš„è¾“å‡ºå¤´
        output_head_dim = hidden_dim
        self.bai_head = self._create_enhanced_output_head(lstm_output_dim * 2)  # å¤šå°ºåº¦æ± åŒ–åç»´åº¦ç¿»å€
        self.shi_head = self._create_enhanced_output_head(lstm_output_dim * 2)
        self.ge_head = self._create_enhanced_output_head(lstm_output_dim * 2)

        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()

    def _create_enhanced_output_head(self, input_dim):
        return nn.Sequential(
            nn.Linear(input_dim, input_dim // 2),
            nn.BatchNorm1d(input_dim // 2),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(input_dim // 2, input_dim // 4),
            nn.GELU(),
            nn.Dropout(0.05),
            nn.Linear(input_dim // 4, 10)
        )

    def _initialize_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
            elif isinstance(module, nn.LSTM):
                for name, param in module.named_parameters():
                    if 'weight' in name:
                        nn.init.orthogonal_(param)
                    elif 'bias' in name:
                        nn.init.constant_(param, 0)
            elif isinstance(module, nn.BatchNorm1d):
                nn.init.constant_(module.weight, 1)
                nn.init.constant_(module.bias, 0)

    def forward(self, x):
        batch_size, seq_len, _ = x.shape

        # é—¨æ§æœºåˆ¶ - ä½¿ç”¨åºåˆ—å¹³å‡
        gate_input = x.mean(dim=1)
        gate_weights = self.gate(gate_input)

        # ä¸“å®¶å¤„ç†
        expert_outputs = []
        x_flat = x.reshape(-1, x.size(-1))

        for i, expert in enumerate(self.experts):
            expert_out = expert(x_flat)
            expert_out = expert_out.reshape(batch_size, seq_len, -1)
            expert_outputs.append(expert_out.unsqueeze(-1))

        # åŠ æƒç»„åˆ
        expert_outputs = torch.cat(expert_outputs, dim=-1)
        weighted_experts = torch.einsum('bsde,be->bsd', expert_outputs, gate_weights)

        # æ®‹å·®è¿æ¥
        residual = self.residual_linear(x)
        weighted_experts = weighted_experts + residual

        # æ—¶åºå¤„ç†
        temporal_out, _ = self.temporal_processor(weighted_experts)
        temporal_out = self.layer_norm1(temporal_out)

        # æ³¨æ„åŠ›æœºåˆ¶
        attn_out, _ = self.attention(temporal_out, temporal_out, temporal_out)
        combined = self.layer_norm2(temporal_out + attn_out)

        # å¤šå°ºåº¦æ± åŒ–
        avg_pool = combined.mean(dim=1)
        max_pool = combined.max(dim=1)[0]
        pooled = torch.cat([avg_pool, max_pool], dim=1)

        # è¾“å‡ºé¢„æµ‹
        bai_logits = self.bai_head(pooled)
        shi_logits = self.shi_head(pooled)
        ge_logits = self.ge_head(pooled)

        return (
            torch.softmax(bai_logits, dim=-1),
            torch.softmax(shi_logits, dim=-1),
            torch.softmax(ge_logits, dim=-1)
        )


class EnhancedAttentionLSTM(BaseModel):
    """å¢å¼ºç‰ˆæ³¨æ„åŠ›LSTMæ¨¡å‹"""

    def __init__(self, input_dim: int, hidden_dim: int = 256, num_layers: int = 3,
                 dropout_rate: float = 0.2, lstm_dropout: float = 0.1):
        super().__init__(input_dim, hidden_dim)

        self.lstm = nn.LSTM(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=lstm_dropout,
            bidirectional=True
        )

        lstm_output_dim = hidden_dim * 2

        self.attention = nn.MultiheadAttention(
            embed_dim=lstm_output_dim,
            num_heads=8,
            dropout=0.1,
            batch_first=True
        )

        self.layer_norm = nn.LayerNorm(lstm_output_dim)

        # å¢å¼ºè¾“å‡ºå¤´
        self.bai_head = self._create_enhanced_output_head(lstm_output_dim)
        self.shi_head = self._create_enhanced_output_head(lstm_output_dim)
        self.ge_head = self._create_enhanced_output_head(lstm_output_dim)

        self._initialize_weights()

    def _create_enhanced_output_head(self, input_dim):
        return nn.Sequential(
            nn.Linear(input_dim, input_dim // 2),
            nn.BatchNorm1d(input_dim // 2),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(input_dim // 2, input_dim // 4),
            nn.GELU(),
            nn.Dropout(0.05),
            nn.Linear(input_dim // 4, 10)
        )

    def _initialize_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
            elif isinstance(module, nn.LSTM):
                for name, param in module.named_parameters():
                    if 'weight' in name:
                        nn.init.orthogonal_(param)
                    elif 'bias' in name:
                        nn.init.constant_(param, 0)
            elif isinstance(module, nn.BatchNorm1d):
                nn.init.constant_(module.weight, 1)
                nn.init.constant_(module.bias, 0)

    def forward(self, x):
        # LSTMå¤„ç†
        lstm_out, (hidden, cell) = self.lstm(x)

        # è‡ªæ³¨æ„åŠ›
        attn_out, attn_weights = self.attention(lstm_out, lstm_out, lstm_out)

        # æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
        normalized_out = self.layer_norm(lstm_out + attn_out)

        # å¤šå°ºåº¦æ± åŒ–
        avg_pool = normalized_out.mean(dim=1)
        max_pool = normalized_out.max(dim=1)[0]
        final_out = torch.cat([avg_pool, max_pool], dim=1)

        # è¾“å‡ºé¢„æµ‹
        bai_logits = self.bai_head(final_out)
        shi_logits = self.shi_head(final_out)
        ge_logits = self.ge_head(final_out)

        return (
            torch.softmax(bai_logits, dim=-1),
            torch.softmax(shi_logits, dim=-1),
            torch.softmax(ge_logits, dim=-1)
        )


class EnhancedTransformer(BaseModel):
    """å¢å¼ºç‰ˆTransformeræ¨¡å‹"""

    def __init__(self, input_dim: int, hidden_dim: int = 256, num_layers: int = 4,
                 num_heads: int = 8, dropout_rate: float = 0.1, attention_dropout: float = 0.05):
        super().__init__(input_dim, hidden_dim)

        # ç¡®ä¿hidden_dimèƒ½è¢«num_headsæ•´é™¤
        assert hidden_dim % num_heads == 0, "hidden_dimå¿…é¡»èƒ½è¢«num_headsæ•´é™¤"

        # ä½ç½®ç¼–ç 
        self.pos_encoding = PositionalEncoding(hidden_dim, max_len=1000)

        # è¾“å…¥æŠ•å½±
        self.input_projection = nn.Linear(input_dim, hidden_dim)

        # Transformerç¼–ç å™¨
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_dim,
            nhead=num_heads,
            dim_feedforward=hidden_dim * 2,
            dropout=dropout_rate,
            activation='gelu',
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)

        # å¢å¼ºçš„æ¦‚ç‡æ ¡å‡†å±‚
        self.calibration = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.GELU(),
            nn.Dropout(0.05)
        )

        # è¾“å‡ºå¤´
        self.bai_head = nn.Linear(hidden_dim // 2, 10)
        self.shi_head = nn.Linear(hidden_dim // 2, 10)
        self.ge_head = nn.Linear(hidden_dim // 2, 10)

        # æ¸©åº¦å‚æ•°ç”¨äºæ¦‚ç‡æ ¡å‡†
        self.temperature = nn.Parameter(torch.ones(1))

        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()

    def _initialize_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
            elif isinstance(module, nn.TransformerEncoderLayer):
                for name, param in module.named_parameters():
                    if 'weight' in name and 'norm' not in name:
                        if 'linear' in name:
                            nn.init.xavier_uniform_(param)
                        else:
                            nn.init.xavier_uniform_(param)
            elif isinstance(module, nn.BatchNorm1d):
                nn.init.constant_(module.weight, 1)
                nn.init.constant_(module.bias, 0)

    def forward(self, x):
        batch_size, seq_len, _ = x.shape

        # è¾“å…¥æŠ•å½±
        x_proj = self.input_projection(x)

        # æ·»åŠ ä½ç½®ç¼–ç 
        x_pos = self.pos_encoding(x_proj)

        # Transformerå¤„ç†
        transformer_out = self.transformer(x_pos)

        # å¤šå°ºåº¦æ± åŒ–
        avg_pool = transformer_out.mean(dim=1)
        max_pool = transformer_out.max(dim=1)[0]
        pooled = torch.cat([avg_pool, max_pool], dim=1)

        # æ¦‚ç‡æ ¡å‡†
        calibrated = self.calibration(pooled)

        # æ¸©åº¦ç¼©æ”¾
        calibrated = calibrated / self.temperature

        # è¾“å‡ºé¢„æµ‹
        bai_logits = self.bai_head(calibrated)
        shi_logits = self.shi_head(calibrated)
        ge_logits = self.ge_head(calibrated)

        return (
            torch.softmax(bai_logits, dim=-1),
            torch.softmax(shi_logits, dim=-1),
            torch.softmax(ge_logits, dim=-1)
        )


class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç """

    def __init__(self, d_model: int, max_len: int = 5000):
        super().__init__()

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                             (-np.log(10000.0) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)

        self.register_buffer('pe', pe)

    def forward(self, x):
        return x + self.pe[:, :x.size(1)]


class EnhancedModelTrainer:
    """å¢å¼ºç‰ˆæ¨¡å‹è®­ç»ƒå™¨"""

    def __init__(self, model: BaseModel, model_name: str, learning_rate: float = EnhancedConstants.LEARNING_RATE):
        self.model = model
        self.model_name = model_name
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)

        # ä½¿ç”¨æ›´å…ˆè¿›çš„ä¼˜åŒ–å™¨
        self.optimizer = optim.AdamW(
            model.parameters(),
            lr=learning_rate,
            weight_decay=1e-4,
            betas=(0.9, 0.999)
        )

        # ä½¿ç”¨ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦
        self.scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.optimizer,
            T_0=EnhancedConstants.WARMUP_EPOCHS,
            T_mult=2,
            eta_min=1e-6
        )

        # æŸå¤±å‡½æ•°
        self.criterion = nn.CrossEntropyLoss()

        # è®­ç»ƒçŠ¶æ€
        self.train_losses = []
        self.val_losses = []
        self.train_accuracies = []
        self.val_accuracies = []

        # æ¢¯åº¦ç´¯ç§¯
        self.gradient_accumulation_steps = EnhancedConstants.GRADIENT_ACCUMULATION_STEPS

        logging.info(f"Enhanced model trainer initialized for {model_name} on device: {self.device}")

    def cross_entropy_with_label_smoothing(self, pred, target, epsilon=EnhancedConstants.LABEL_SMOOTHING):
        """æ ‡ç­¾å¹³æ»‘çš„äº¤å‰ç†µæŸå¤±"""
        n_classes = pred.size(1)
        one_hot = torch.zeros_like(pred).scatter(1, target.unsqueeze(1), 1)
        one_hot = one_hot * (1 - epsilon) + epsilon / n_classes
        log_prb = torch.log_softmax(pred, dim=1)
        loss = -(one_hot * log_prb).sum(dim=1).mean()
        return loss

    def calculate_accuracy(self, probs, targets):
        """è®¡ç®—å‡†ç¡®ç‡"""
        _, predicted = torch.max(probs, 1)
        correct = (predicted == targets).float().sum()
        return correct.item() / targets.size(0)

    def train_epoch(self, train_loader: DataLoader, epoch: int) -> Tuple[float, float]:
        """å¢å¼ºçš„è®­ç»ƒepoch"""
        self.model.train()
        total_loss = 0
        total_acc = 0
        total_batches = len(train_loader)

        if total_batches == 0:
            raise DataValidationError("è®­ç»ƒæ•°æ®ä¸ºç©º")

        self.optimizer.zero_grad()

        with ProgressBar(total_batches, desc=f"Epoch {epoch} Training") as pbar:
            for batch_idx, (data, bai_target, shi_target, ge_target) in enumerate(train_loader):
                data = data.to(self.device)
                bai_target = bai_target.to(self.device).squeeze()
                shi_target = shi_target.to(self.device).squeeze()
                ge_target = ge_target.to(self.device).squeeze()

                bai_probs, shi_probs, ge_probs = self.model(data)

                # ä½¿ç”¨æ ‡ç­¾å¹³æ»‘çš„æŸå¤±å‡½æ•°
                bai_loss = self.cross_entropy_with_label_smoothing(bai_probs, bai_target)
                shi_loss = self.cross_entropy_with_label_smoothing(shi_probs, shi_target)
                ge_loss = self.cross_entropy_with_label_smoothing(ge_probs, ge_target)

                loss = (bai_loss + shi_loss + ge_loss) / 3
                loss = loss / self.gradient_accumulation_steps
                loss.backward()

                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=0.5)

                if (batch_idx + 1) % self.gradient_accumulation_steps == 0:
                    self.optimizer.step()
                    self.optimizer.zero_grad()
                    self.scheduler.step(epoch + batch_idx / total_batches)

                total_loss += loss.item() * self.gradient_accumulation_steps

                # è®¡ç®—å‡†ç¡®ç‡
                bai_acc = self.calculate_accuracy(bai_probs, bai_target)
                shi_acc = self.calculate_accuracy(shi_probs, shi_target)
                ge_acc = self.calculate_accuracy(ge_probs, ge_target)
                batch_acc = (bai_acc + shi_acc + ge_acc) / 3
                total_acc += batch_acc

                pbar.update(1, loss=loss.item(), accuracy=f"{batch_acc:.4f}")

        # å¤„ç†å‰©ä½™çš„æ¢¯åº¦
        if total_batches % self.gradient_accumulation_steps != 0:
            self.optimizer.step()
            self.optimizer.zero_grad()

        avg_loss = total_loss / total_batches
        avg_acc = total_acc / total_batches
        self.train_losses.append(avg_loss)
        self.train_accuracies.append(avg_acc)
        return avg_loss, avg_acc

    def validate(self, val_loader: DataLoader) -> Tuple[float, float]:
        """éªŒè¯"""
        self.model.eval()
        total_loss = 0
        total_acc = 0
        total_batches = len(val_loader)

        if total_batches == 0:
            raise DataValidationError("éªŒè¯æ•°æ®ä¸ºç©º")

        with torch.no_grad():
            with ProgressBar(total_batches, desc="Validating") as pbar:
                for data, bai_target, shi_target, ge_target in val_loader:
                    data = data.to(self.device)
                    bai_target = bai_target.to(self.device).squeeze()
                    shi_target = shi_target.to(self.device).squeeze()
                    ge_target = ge_target.to(self.device).squeeze()

                    bai_probs, shi_probs, ge_probs = self.model(data)

                    bai_loss = self.cross_entropy_with_label_smoothing(bai_probs, bai_target)
                    shi_loss = self.cross_entropy_with_label_smoothing(shi_probs, shi_target)
                    ge_loss = self.cross_entropy_with_label_smoothing(ge_probs, ge_target)

                    loss = (bai_loss + shi_loss + ge_loss) / 3
                    total_loss += loss.item()

                    # è®¡ç®—å‡†ç¡®ç‡
                    bai_acc = self.calculate_accuracy(bai_probs, bai_target)
                    shi_acc = self.calculate_accuracy(shi_probs, shi_target)
                    ge_acc = self.calculate_accuracy(ge_probs, ge_target)
                    batch_acc = (bai_acc + shi_acc + ge_acc) / 3
                    total_acc += batch_acc

                    pbar.update(1, loss=loss.item(), accuracy=f"{batch_acc:.4f}")

        avg_loss = total_loss / total_batches
        avg_acc = total_acc / total_batches
        self.val_losses.append(avg_loss)
        self.val_accuracies.append(avg_acc)
        return avg_loss, avg_acc

    def train(self, train_loader: DataLoader, val_loader: DataLoader,
              epochs: int = EnhancedConstants.MAX_EPOCHS,
              patience: int = EnhancedConstants.PATIENCE) -> Dict[str, Any]:
        """å®Œæ•´è®­ç»ƒè¿‡ç¨‹"""
        best_val_loss = float('inf')
        best_val_acc = 0
        patience_counter = 0
        best_model_state = None

        logging.info(f"å¼€å§‹è®­ç»ƒ {self.model_name}...")
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒ {self.model_name}...")
        print(f"ğŸ“Š è®­ç»ƒæ ·æœ¬: {len(train_loader.dataset)}, éªŒè¯æ ·æœ¬: {len(val_loader.dataset)}")
        print(f"âš™ï¸  ä½¿ç”¨å¢å¼ºè®­ç»ƒç­–ç•¥: æ ‡ç­¾å¹³æ»‘={EnhancedConstants.LABEL_SMOOTHING}, "
              f"æ¢¯åº¦ç´¯ç§¯={self.gradient_accumulation_steps}")

        start_time = time.time()

        for epoch in range(1, epochs + 1):
            try:
                train_loss, train_acc = self.train_epoch(train_loader, epoch)
                val_loss, val_acc = self.validate(val_loader)

                current_lr = self.optimizer.param_groups[0]['lr']

                print(f'Epoch: {epoch}/{epochs}\t'
                      f'è®­ç»ƒæŸå¤±: {train_loss:.6f}\tè®­ç»ƒå‡†ç¡®ç‡: {train_acc:.4f}\t'
                      f'éªŒè¯æŸå¤±: {val_loss:.6f}\téªŒè¯å‡†ç¡®ç‡: {val_acc:.4f}\t'
                      f'å­¦ä¹ ç‡: {current_lr:.2e}')

                # æ—©åœæ£€æŸ¥
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    best_val_acc = val_acc
                    patience_counter = 0
                    best_model_state = self.model.state_dict().copy()
                    print(f'ğŸ¯ å‘ç°æ›´å¥½çš„æ¨¡å‹! éªŒè¯æŸå¤±: {val_loss:.6f}, éªŒè¯å‡†ç¡®ç‡: {val_acc:.4f}')
                else:
                    patience_counter += 1
                    print(f'â³ æ—©åœè®¡æ•°: {patience_counter}/{patience}')

                if patience_counter >= patience:
                    print(f'ğŸ›‘ æ—©åœè§¦å‘! åœ¨epoch {epoch}åœæ­¢è®­ç»ƒ')
                    break

            except Exception as e:
                logging.error(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
                print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
                break

        # åŠ è½½æœ€ä½³æ¨¡å‹
        if best_model_state is not None:
            self.model.load_state_dict(best_model_state)

        training_time = time.time() - start_time

        result = {
            'best_val_loss': best_val_loss,
            'best_val_acc': best_val_acc,
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'train_accuracies': self.train_accuracies,
            'val_accuracies': self.val_accuracies,
            'final_epoch': epoch,
            'training_time': training_time
        }

        logging.info(f"è®­ç»ƒå®Œæˆ: {result}")
        return result


class EnhancedHyperparameterOptimizer:
    """å¢å¼ºç‰ˆè¶…å‚æ•°ä¼˜åŒ–å™¨"""

    def __init__(self, predictor, model_type):
        self.predictor = predictor
        self.model_type = model_type
        self.study = None
        self.best_params = None

        # å¢å¼ºçš„æœç´¢ç©ºé—´
        self.search_spaces = {
            'temporal_moe': {
                'hidden_dim': {'type': 'int', 'low': 192, 'high': 512},
                'num_experts': {'type': 'int', 'low': 6, 'high': 16},
                'learning_rate': {'type': 'float', 'low': 1e-5, 'high': 5e-4, 'log': True},
                'dropout_rate': {'type': 'float', 'low': 0.1, 'high': 0.4},
                'expert_dropout': {'type': 'float', 'low': 0.05, 'high': 0.2}
            },
            'attention_lstm': {
                'hidden_dim': {'type': 'int', 'low': 192, 'high': 512},
                'num_layers': {'type': 'int', 'low': 2, 'high': 5},
                'learning_rate': {'type': 'float', 'low': 1e-5, 'high': 5e-4, 'log': True},
                'dropout_rate': {'type': 'float', 'low': 0.1, 'high': 0.4},
                'lstm_dropout': {'type': 'float', 'low': 0.05, 'high': 0.2}
            },
            'transformer': {
                'hidden_dim': {'type': 'int', 'low': 192, 'high': 512},
                'num_layers': {'type': 'int', 'low': 3, 'high': 8},
                'num_heads': {'type': 'int', 'low': 4, 'high': 12},
                'learning_rate': {'type': 'float', 'low': 1e-5, 'high': 5e-4, 'log': True},
                'dropout_rate': {'type': 'float', 'low': 0.1, 'high': 0.3},
                'attention_dropout': {'type': 'float', 'low': 0.05, 'high': 0.15}
            }
        }

    def optimize(self, n_trials: int = EnhancedConstants.OPTIMIZATION_TRIALS,
                 timeout: int = EnhancedConstants.OPTIMIZATION_TIMEOUT) -> Dict[str, Any]:
        """æ‰§è¡Œè¶…å‚æ•°ä¼˜åŒ–"""
        print(f"ğŸ” å¼€å§‹å¢å¼ºè¶…å‚æ•°ä¼˜åŒ–: {self.model_type}")
        print(f"ğŸ¯ è¯•éªŒæ¬¡æ•°: {n_trials}, è¶…æ—¶: {timeout}ç§’")

        # åˆ›å»ºOptunaç ”ç©¶
        self.study = optuna.create_study(
            direction='maximize',
            sampler=TPESampler(seed=42)
        )

        # æ‰§è¡Œä¼˜åŒ–
        self.study.optimize(
            lambda trial: self._objective(trial),
            n_trials=n_trials,
            timeout=timeout,
            show_progress_bar=True
        )

        # ä¿å­˜æœ€ä½³å‚æ•°
        self.best_params = self.study.best_params
        self._save_optimization_results()

        return self.best_params

    def _objective(self, trial: Trial) -> float:
        """ä¼˜åŒ–ç›®æ ‡å‡½æ•°"""
        try:
            # è·å–æœç´¢ç©ºé—´
            search_space = self.search_spaces[self.model_type]
            params = {}

            # é‡‡æ ·è¶…å‚æ•°
            for param_name, config in search_space.items():
                if config['type'] == 'int':
                    params[param_name] = trial.suggest_int(param_name, config['low'], config['high'])
                elif config['type'] == 'float':
                    params[param_name] = trial.suggest_float(
                        param_name, config['low'], config['high'], log=config.get('log', False)
                    )

            # å¯¹äºtransformeræ¨¡å‹ï¼Œç¡®ä¿hidden_dimèƒ½è¢«num_headsæ•´é™¤
            if self.model_type == 'transformer':
                hidden_dim = params['hidden_dim']
                num_heads = params['num_heads']
                if hidden_dim % num_heads != 0:
                    params['hidden_dim'] = (hidden_dim // num_heads) * num_heads

            # ä½¿ç”¨æ—¶é—´åºåˆ—äº¤å‰éªŒè¯è¯„ä¼°å‚æ•°
            avg_val_acc = self._cross_validate(params)

            return avg_val_acc

        except Exception as e:
            print(f"âŒ è¶…å‚æ•°è¯•éªŒå¤±è´¥: {e}")
            return -1.0

    def _cross_validate(self, params: Dict[str, Any], n_splits: int = 3) -> float:
        """æ—¶é—´åºåˆ—äº¤å‰éªŒè¯"""
        if not self.predictor.is_loaded:
            raise DataValidationError("è¯·å…ˆåŠ è½½æ•°æ®")

        sequence_length = EnhancedConstants.DEFAULT_SEQUENCE_LENGTH
        if len(self.predictor.data) < sequence_length * 2:
            sequence_length = len(self.predictor.data) // 2

        tscv = TimeSeriesSplit(n_splits=min(n_splits, len(self.predictor.data) // sequence_length))
        val_accuracies = []

        data_array = np.arange(len(self.predictor.data))

        for train_idx, val_idx in tscv.split(data_array):
            try:
                train_data = self.predictor.data.iloc[train_idx]
                val_data = self.predictor.data.iloc[val_idx]

                if len(train_data) < sequence_length or len(val_data) < 1:
                    continue

                # ä½¿ç”¨å¢å¼ºæ•°æ®é›†
                train_dataset = EnhancedFC3DDataset(train_data, sequence_length,
                                                    feature_columns=None, fit_scaler=True)
                val_dataset = EnhancedFC3DDataset(val_data, sequence_length,
                                                  feature_columns=train_dataset.get_feature_columns(),
                                                  fit_scaler=False, external_scaler=train_dataset.get_scaler())

                train_loader = DataLoader(train_dataset, batch_size=EnhancedConstants.BATCH_SIZE,
                                          shuffle=True, num_workers=0)
                val_loader = DataLoader(val_dataset, batch_size=EnhancedConstants.BATCH_SIZE,
                                        shuffle=False, num_workers=0)

                if len(train_loader) == 0 or len(val_loader) == 0:
                    continue

                # åˆ›å»ºæ¨¡å‹
                input_dim = train_dataset.get_feature_dimension()
                config = self.predictor.model_configs[self.model_type]

                model_params = {k: v for k, v in params.items()
                                if k in ['hidden_dim', 'num_layers', 'num_heads', 'num_experts',
                                         'dropout_rate', 'expert_dropout', 'lstm_dropout', 'attention_dropout']}
                # âœ… ä¿®å¤ï¼šç¡®ä¿è¾“å…¥ç»´åº¦æ­£ç¡®ä¼ å…¥
if model_type == 'attention_lstm':
    model = EnhancedAttentionLSTM(
        input_dim=input_dim,
        hidden_dim=model_params.get('hidden_dim', 256),
        num_layers=model_params.get('num_layers', 3),
        dropout_rate=model_params.get('dropout_rate', 0.2),
        lstm_dropout=model_params.get('lstm_dropout', 0.1)
    )
elif model_type == 'transformer':
    model = EnhancedTransformer(
        input_dim=input_dim,
        hidden_dim=model_params.get('hidden_dim', 256),
        num_layers=model_params.get('num_layers', 4),
        num_heads=model_params.get('num_heads', 8),
        dropout_rate=model_params.get('dropout_rate', 0.1),
        attention_dropout=model_params.get('attention_dropout', 0.05)
    )
else:
    model = config['class'](input_dim=input_dim, **model_params

                # åˆ›å»ºå¢å¼ºè®­ç»ƒå™¨
                trainer = EnhancedModelTrainer(model, f"{self.model_type}_cv")
                if 'learning_rate' in params:
                    trainer.optimizer = optim.AdamW(
                        model.parameters(),
                        lr=params['learning_rate'],
                        weight_decay=1e-4,
                        betas=(0.9, 0.999)
                    )

                # å¿«é€Ÿè¯„ä¼°
                model.train()
                for epoch in range(3):
                    total_val_acc = 0
                    total_batches = 0

                    for data, bai_target, shi_target, ge_target in val_loader:
                        data = data.to(trainer.device)
                        bai_target = bai_target.to(trainer.device).squeeze()
                        shi_target = shi_target.to(trainer.device).squeeze()
                        ge_target = ge_target.to(trainer.device).squeeze()

                        bai_probs, shi_probs, ge_probs = model(data)

                        bai_acc = trainer.calculate_accuracy(bai_probs, bai_target)
                        shi_acc = trainer.calculate_accuracy(shi_probs, shi_target)
                        ge_acc = trainer.calculate_accuracy(ge_probs, ge_target)

                        batch_acc = (bai_acc + shi_acc + ge_acc) / 3
                        total_val_acc += batch_acc
                        total_batches += 1

                    if total_batches > 0:
                        avg_val_acc = total_val_acc / total_batches
                    else:
                        avg_val_acc = 0

                val_accuracies.append(avg_val_acc)

            except Exception as e:
                print(f"âš ï¸ äº¤å‰éªŒè¯æŠ˜å å¤±è´¥: {e}")
                continue

        if val_accuracies:
            return np.mean(val_accuracies)
        else:
            return 0.0

    def _save_optimization_results(self):
        """ä¿å­˜ä¼˜åŒ–ç»“æœ"""
        if self.study and self.best_params:
            results = {
                'model_type': self.model_type,
                'best_params': self.best_params,
                'best_value': self.study.best_value,
                'completed_trials': len(self.study.trials),
                'timestamp': datetime.now().isoformat(),
                'trials_summary': [
                    {
                        'number': trial.number,
                        'value': trial.value,
                        'params': trial.params,
                        'state': str(trial.state)
                    }
                    for trial in self.study.trials
                ]
            }

            filename = f"{self.model_type}_enhanced_optimization_results.json"
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)

            print(f"ğŸ’¾ ä¼˜åŒ–ç»“æœå·²ä¿å­˜: {filename}")
            self._display_optimization_summary()

    def _display_optimization_summary(self):
        """æ˜¾ç¤ºä¼˜åŒ–æ€»ç»“"""
        print(f"\n{'=' * 80}")
        print(f"ğŸ‰ {self.model_type.upper()} å¢å¼ºè¶…å‚æ•°ä¼˜åŒ–å®Œæˆ!")
        print(f"{'=' * 80}")
        print(f"ğŸ† æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {self.study.best_value:.4f}")
        print(f"ğŸ”§ æœ€ä½³è¶…å‚æ•°:")
        for param, value in self.best_params.items():
            print(f"   {param}: {value}")
        print(f"ğŸ“Š å®Œæˆè¯•éªŒæ•°: {len(self.study.trials)}")
        print(f"{'=' * 80}")


class EnhancedFC3DPredictor:
    """å¢å¼ºç‰ˆFC3Dé¢„æµ‹ç³»ç»Ÿä¸»ç±»"""

    def __init__(self):
        self.data = None
        self.models = {}
        self.current_period = None
        self.is_loaded = False
        self.feature_columns = None
        self.optimization_results = {}
        self.current_scaler = None
        self.performance_tracker = PerformanceTracker()

        # åˆå§‹åŒ–æ¨¡å‹çŠ¶æ€
        self._initialize_models()

        # å¢å¼ºæ¨¡å‹é…ç½®
        self.model_configs = {
            'temporal_moe': {
                'class': EnhancedTemporalMoE,
                'hidden_dim': 256,
                'num_experts': 8,
                'dropout_rate': 0.2,
                'expert_dropout': 0.1
            },
            'attention_lstm': {
                'class': EnhancedAttentionLSTM,
                'hidden_dim': 256,
                'num_layers': 3,
                'dropout_rate': 0.2,
                'lstm_dropout': 0.1
            },
            'transformer': {
                'class': EnhancedTransformer,
                'hidden_dim': 256,
                'num_layers': 4,
                'num_heads': 8,
                'dropout_rate': 0.1,
                'attention_dropout': 0.05
            }
        }

        # åŠ è½½é…ç½®å’Œæ€§èƒ½æ•°æ®
        try:
            ConfigManager.load_config(self)
            self.performance_tracker.load_performance_data()
        except Exception as e:
            logging.warning(f"åŠ è½½é…ç½®æˆ–æ€§èƒ½æ•°æ®å¤±è´¥: {e}")

        logging.info("å¢å¼ºç‰ˆFC3Dé¢„æµ‹ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")

    def _initialize_models(self):
        """åˆå§‹åŒ–æ¨¡å‹çŠ¶æ€"""
        self.models = {}
        for model_type in ['temporal_moe', 'attention_lstm', 'transformer']:
            self.models[model_type] = {
                'model': None,
                'trainer': None,
                'info': {},
                'is_trained': False,
                'optimized_params': None
            }

    def load_data(self, file_path: str) -> bool:
        """åŠ è½½æ•°æ®"""
        try:
            logging.info(f"æ­£åœ¨åŠ è½½æ•°æ®: {file_path}")
            print(f"ğŸ“‚ æ­£åœ¨åŠ è½½æ•°æ®: {file_path}")

            if not os.path.exists(file_path):
                raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

            self.data = pd.read_csv(file_path)

            # æ•°æ®éªŒè¯
            validation_result = DataQualityValidator.validate_data_structure(self.data)

            if not validation_result['is_valid']:
                error_msg = "æ•°æ®éªŒè¯å¤±è´¥:\n" + "\n".join(validation_result['errors'])
                raise DataValidationError(error_msg)

            # æ˜¾ç¤ºæ•°æ®è´¨é‡æŠ¥å‘Š
            report = DataQualityValidator.generate_data_report(self.data)
            print(report)

            # æ•°æ®æ’åº
            self.data = self.data.sort_values('æœŸå·').reset_index(drop=True)

            # æ£€æŸ¥æ•°æ®é‡
            if len(self.data) < EnhancedConstants.MIN_DATA_POINTS:
                print(
                    f"âš ï¸  è­¦å‘Š: æ•°æ®é‡è¾ƒå°‘({len(self.data)}æœŸ)ï¼Œå»ºè®®è‡³å°‘{EnhancedConstants.MIN_DATA_POINTS}æœŸæ•°æ®ä»¥è·å¾—æ›´å¥½æ•ˆæœ")

            # è®¾ç½®å½“å‰æœŸå·
            self.current_period = self.data['æœŸå·'].max()

            # åˆå§‹åŒ–ç‰¹å¾åˆ— - ä½¿ç”¨å¢å¼ºæ•°æ®é›†
            temp_dataset = EnhancedFC3DDataset(self.data, sequence_length=10)
            self.feature_columns = temp_dataset.get_feature_columns()

            # é‡ç½®scaler
            self.current_scaler = None

            print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ! å…±{len(self.data)}æœŸæ•°æ®, æœ€æ–°æœŸå·: {self.current_period}")
            print(f"ğŸ“Š å¢å¼ºç‰¹å¾æ•°é‡: {len(self.feature_columns)}")
            self.is_loaded = True

            # ä¿å­˜é…ç½®
            ConfigManager.save_config(self)

            logging.info(f"æ•°æ®åŠ è½½æˆåŠŸ: {len(self.data)}è¡Œæ•°æ®")
            return True

        except Exception as e:
            error_msg = f"æ•°æ®åŠ è½½å¤±è´¥: {e}"
            logging.error(error_msg)
            print(f"âŒ {error_msg}")
            return False

    def prepare_datasets(self, sequence_length: int = EnhancedConstants.DEFAULT_SEQUENCE_LENGTH) -> Tuple[
        EnhancedFC3DDataset, EnhancedFC3DDataset]:
        """å‡†å¤‡è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†"""
        if not self.is_loaded:
            raise DataValidationError("è¯·å…ˆåŠ è½½æ•°æ®")

        if len(self.data) < sequence_length + EnhancedConstants.MIN_SEQUENCE_LENGTH:
            raise DataValidationError(
                f"æ•°æ®é‡ä¸è¶³ï¼Œè‡³å°‘éœ€è¦{sequence_length + EnhancedConstants.MIN_SEQUENCE_LENGTH}æœŸæ•°æ®")

        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        split_idx = int((1 - EnhancedConstants.VALIDATION_SPLIT) * len(self.data))
        train_data = self.data.iloc[:split_idx]
        val_data = self.data.iloc[split_idx:]

        # ä½¿ç”¨å¢å¼ºæ•°æ®é›†
        train_dataset = EnhancedFC3DDataset(train_data, sequence_length, self.feature_columns, fit_scaler=True)
        val_dataset = EnhancedFC3DDataset(val_data, sequence_length, self.feature_columns, fit_scaler=False,
                                          external_scaler=train_dataset.get_scaler())

        # ä¿å­˜scalerçŠ¶æ€
        self.current_scaler = train_dataset.get_scaler()

        print(f"ğŸ“Š å¢å¼ºæ•°æ®é›†åˆ’åˆ†: è®­ç»ƒé›†{len(train_dataset)}æ ·æœ¬, éªŒè¯é›†{len(val_dataset)}æ ·æœ¬")
        logging.info(f"å¢å¼ºæ•°æ®é›†åˆ’åˆ†å®Œæˆ: è®­ç»ƒé›†{len(train_dataset)}, éªŒè¯é›†{len(val_dataset)}")

        return train_dataset, val_dataset

    def train_model(self, model_type: str, use_optimized_params: bool = False) -> bool:
        """è®­ç»ƒæŒ‡å®šç±»å‹çš„æ¨¡å‹"""
        if not self.is_loaded:
            print("âŒ è¯·å…ˆåŠ è½½æ•°æ®")
            return False

        if model_type not in self.model_configs:
            print(f"âŒ æœªçŸ¥çš„æ¨¡å‹ç±»å‹: {model_type}")
            return False

        try:
            logging.info(f"å¼€å§‹è®­ç»ƒå¢å¼ºæ¨¡å‹: {model_type}")

            # å‡†å¤‡æ•°æ®
            train_dataset, val_dataset = self.prepare_datasets()
            train_loader = DataLoader(train_dataset, batch_size=EnhancedConstants.BATCH_SIZE, shuffle=True,
                                      num_workers=0)
            val_loader = DataLoader(val_dataset, batch_size=EnhancedConstants.BATCH_SIZE, shuffle=False, num_workers=0)

            # è·å–è¾“å…¥ç»´åº¦
            input_dim = train_dataset.get_feature_dimension()

            # åˆ›å»ºæ¨¡å‹
            config = self.model_configs[model_type]

            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ä¼˜åŒ–å‚æ•°
            if use_optimized_params and model_type in self.optimization_results:
                optimized_params = self.optimization_results[model_type]['best_params']
                model_params = {k: v for k, v in optimized_params.items()
                                if k in ['hidden_dim', 'num_layers', 'num_heads', 'num_experts',
                                         'dropout_rate', 'expert_dropout', 'lstm_dropout', 'attention_dropout']}
                learning_rate = optimized_params.get('learning_rate', EnhancedConstants.LEARNING_RATE)
                print(f"ğŸ¯ ä½¿ç”¨ä¼˜åŒ–å‚æ•°è®­ç»ƒå¢å¼ºæ¨¡å‹: {model_params}")
            else:
                model_params = {k: v for k, v in config.items() if k != 'class'}
                learning_rate = EnhancedConstants.LEARNING_RATE

            model = config['class'](input_dim=input_dim, **model_params)

            # ä½¿ç”¨å¢å¼ºè®­ç»ƒå™¨è®­ç»ƒæ¨¡å‹
            trainer = EnhancedModelTrainer(model, model_type, learning_rate=learning_rate)
            training_info = trainer.train(train_loader, val_loader)

            # ä¿å­˜æ¨¡å‹
            self.models[model_type] = {
                'model': model,
                'trainer': trainer,
                'info': training_info,
                'is_trained': True,
                'optimized_params': model_params if use_optimized_params else None
            }

            # æ˜¾ç¤ºè¯¦ç»†è®­ç»ƒä¿¡æ¯
            self._display_training_summary(model_type, training_info, use_optimized_params)

            # ä¿å­˜æ¨¡å‹åˆ°æ–‡ä»¶
            self._save_model(model_type, input_dim)

            # ä¿å­˜é…ç½®
            ConfigManager.save_config(self)

            logging.info(f"å¢å¼ºæ¨¡å‹è®­ç»ƒå®Œæˆ: {model_type}")
            return True

        except Exception as e:
            error_msg = f"å¢å¼ºæ¨¡å‹è®­ç»ƒå¤±è´¥: {e}"
            logging.error(error_msg)
            print(f"âŒ {error_msg}")
            import traceback
            traceback.print_exc()
            return False

    def _display_training_summary(self, model_type: str, training_info: Dict, use_optimized_params: bool):
        """æ˜¾ç¤ºè®­ç»ƒæ€»ç»“"""
        best_val_loss = training_info['best_val_loss']
        best_val_acc = training_info['best_val_acc']
        training_time = training_info['training_time']

        print(f"\n{'=' * 60}")
        if use_optimized_params:
            print(f"ğŸ‰ {model_type.upper()} å¢å¼ºæ¨¡å‹(ä¼˜åŒ–å‚æ•°)è®­ç»ƒå®Œæˆ!")
        else:
            print(f"ğŸ‰ {model_type.upper()} å¢å¼ºæ¨¡å‹è®­ç»ƒå®Œæˆ!")
        print(f"{'=' * 60}")
        print(f"ğŸ“ˆ æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
        print(f"ğŸ¯ æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}")
        print(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {training_time:.2f}ç§’")
        print(f"ğŸ”„ è®­ç»ƒè½®æ¬¡: {training_info['final_epoch']}")

        if training_info['train_losses']:
            print(f"ğŸ“‰ æœ€ç»ˆè®­ç»ƒæŸå¤±: {training_info['train_losses'][-1]:.6f}")
        else:
            print(f"ğŸ“‰ æœ€ç»ˆè®­ç»ƒæŸå¤±: N/A")

        if training_info['train_accuracies']:
            print(f"ğŸ“Š æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡: {training_info['train_accuracies'][-1]:.4f}")
        else:
            print(f"ğŸ“Š æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡: N/A")

        print(f"âš™ï¸  è®­ç»ƒç­–ç•¥: æ ‡ç­¾å¹³æ»‘={EnhancedConstants.LABEL_SMOOTHING}, "
              f"æ¢¯åº¦ç´¯ç§¯={EnhancedConstants.GRADIENT_ACCUMULATION_STEPS}")
        print(f"{'=' * 60}")

    def _save_model(self, model_type: str, input_dim: int):
        """ä¿å­˜æ¨¡å‹åˆ°æ–‡ä»¶"""
        if model_type in self.models and self.models[model_type]['is_trained']:
            filename = f"{model_type}_enhanced_model.pth"
            torch.save({
                'model_state_dict': self.models[model_type]['model'].state_dict(),
                'training_info': self.models[model_type]['info'],
                'input_dim': input_dim,
                'feature_columns': self.feature_columns,
                'timestamp': datetime.now().isoformat(),
                'model_type': model_type,
                'optimized_params': self.models[model_type].get('optimized_params')
            }, filename)
            print(f"ğŸ’¾ å¢å¼ºæ¨¡å‹å·²ä¿å­˜: {filename}")
            logging.info(f"å¢å¼ºæ¨¡å‹ä¿å­˜æˆåŠŸ: {filename}")

    def load_existing_models(self) -> int:
        """åŠ è½½å·²æœ‰æ¨¡å‹ï¼Œè¿”å›æˆåŠŸåŠ è½½çš„æ¨¡å‹æ•°é‡"""
        if not self.is_loaded:
            print("âŒ è¯·å…ˆåŠ è½½æ•°æ®")
            return 0

        model_files = {
            'temporal_moe': 'temporal_moe_enhanced_model.pth',
            'attention_lstm': 'attention_lstm_enhanced_model.pth',
            'transformer': 'transformer_enhanced_model.pth'
        }

        loaded_count = 0

        # å…ˆå‡†å¤‡æ•°æ®é›†è·å–ç‰¹å¾ç»´åº¦
        try:
            train_dataset, _ = self.prepare_datasets()
            input_dim = train_dataset.get_feature_dimension()
            logging.info(f"è·å–ç‰¹å¾ç»´åº¦: {input_dim}")
        except Exception as e:
            print(f"âŒ æ— æ³•è·å–ç‰¹å¾ç»´åº¦: {e}")
            return 0

        for model_type, filename in model_files.items():
            if os.path.exists(filename):
                try:
                    print(f"ğŸ”„ æ­£åœ¨åŠ è½½ {model_type} å¢å¼ºæ¨¡å‹...")

                    # æ£€æŸ¥æ–‡ä»¶å®Œæ•´æ€§
                    file_size = os.path.getsize(filename)
                    if file_size == 0:
                        print(f"âš ï¸ æ¨¡å‹æ–‡ä»¶ä¸ºç©º: {filename}")
                        continue

                    # åˆ›å»ºæ¨¡å‹æ¶æ„
                    config = self.model_configs[model_type]
                    model = config['class'](
                        input_dim=input_dim,
                        **{k: v for k, v in config.items() if k != 'class'}
                    )

                    # åŠ è½½æƒé‡
                    checkpoint = torch.load(filename, map_location='cpu')
                    model.load_state_dict(checkpoint['model_state_dict'])

                    # æ›´æ–°æ¨¡å‹çŠ¶æ€
                    self.models[model_type] = {
                        'model': model,
                        'info': checkpoint.get('training_info', {}),
                        'is_trained': True,
                        'optimized_params': checkpoint.get('optimized_params')
                    }

                    print(f"âœ… æˆåŠŸåŠ è½½ {model_type} å¢å¼ºæ¨¡å‹")
                    loaded_count += 1
                    logging.info(f"å¢å¼ºæ¨¡å‹åŠ è½½æˆåŠŸ: {model_type}")

                except Exception as e:
                    error_msg = f"åŠ è½½ {model_type} å¢å¼ºæ¨¡å‹å¤±è´¥: {e}"
                    print(f"âŒ {error_msg}")
                    logging.error(error_msg)
                    # é‡ç½®æ¨¡å‹çŠ¶æ€
                    self.models[model_type] = {
                        'model': None,
                        'info': {},
                        'is_trained': False,
                        'optimized_params': None
                    }
            else:
                print(f"âš ï¸  {model_type} å¢å¼ºæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {filename}")
                if model_type not in self.models:
                    self.models[model_type] = {
                        'model': None,
                        'info': {},
                        'is_trained': False,
                        'optimized_params': None
                    }

        # ä¿å­˜é…ç½®
        ConfigManager.save_config(self)

        print(f"\nğŸ“¥ æ€»å…±æˆåŠŸåŠ è½½äº† {loaded_count} ä¸ªå¢å¼ºæ¨¡å‹")
        return loaded_count

    def optimize_hyperparameters(self, model_type: str,
                                 n_trials: int = EnhancedConstants.OPTIMIZATION_TRIALS,
                                 timeout: int = EnhancedConstants.OPTIMIZATION_TIMEOUT) -> bool:
        """æ‰§è¡Œå¢å¼ºè¶…å‚æ•°ä¼˜åŒ–"""
        if model_type not in self.model_configs:
            print(f"âŒ æœªçŸ¥çš„æ¨¡å‹ç±»å‹: {model_type}")
            return False

        if not self.is_loaded:
            print("âŒ è¯·å…ˆåŠ è½½æ•°æ®")
            return False

        try:
            print(f"ğŸ”¬ å¼€å§‹å¢å¼ºè¶…å‚æ•°ä¼˜åŒ–: {model_type}")
            print(f"ğŸ¯ ç›®æ ‡: é€šè¿‡ {n_trials} æ¬¡è¯•éªŒæ‰¾åˆ°æœ€ä¼˜è¶…å‚æ•°")

            # åˆ›å»ºå¢å¼ºä¼˜åŒ–å™¨
            optimizer = EnhancedHyperparameterOptimizer(self, model_type)

            # æ‰§è¡Œä¼˜åŒ–
            best_params = optimizer.optimize(n_trials=n_trials, timeout=timeout)

            # ä¿å­˜ä¼˜åŒ–ç»“æœ
            self.optimization_results[model_type] = {
                'best_params': best_params,
                'best_score': optimizer.study.best_value if optimizer.study else 0
            }

            # ä½¿ç”¨æœ€ä¼˜å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹
            print(f"ğŸš€ ä½¿ç”¨æœ€ä¼˜å‚æ•°è®­ç»ƒæœ€ç»ˆå¢å¼ºæ¨¡å‹...")
            success = self.train_model(model_type, use_optimized_params=True)

            # ä¿å­˜é…ç½®
            ConfigManager.save_config(self)

            return success

        except Exception as e:
            error_msg = f"å¢å¼ºè¶…å‚æ•°ä¼˜åŒ–å¤±è´¥: {e}"
            print(f"âŒ {error_msg}")
            logging.error(error_msg)
            import traceback
            traceback.print_exc()
            return False

    def predict_next_period(self):
        """é¢„æµ‹ä¸‹ä¸€æœŸå·ç """
        if not self.is_loaded:
            print("âŒ è¯·å…ˆåŠ è½½æ•°æ®")
            return

        # æ£€æŸ¥æ˜¯å¦æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹
        trained_models = [name for name, info in self.models.items()
                          if info.get('is_trained', False)]

        if not trained_models:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„è®­ç»ƒå¥½çš„å¢å¼ºæ¨¡å‹ï¼Œè¯·å…ˆè®­ç»ƒæˆ–åŠ è½½æ¨¡å‹")
            return

        try:
            # å‡†å¤‡æœ€æ–°æ•°æ®
            sequence_length = EnhancedConstants.DEFAULT_SEQUENCE_LENGTH
            if len(self.data) < sequence_length:
                sequence_length = len(self.data)
                if sequence_length < EnhancedConstants.MIN_SEQUENCE_LENGTH:
                    print(f"âŒ æ•°æ®ä¸è¶³ï¼Œè‡³å°‘éœ€è¦{EnhancedConstants.MIN_SEQUENCE_LENGTH}æœŸæ•°æ®")
                    return

            # å–æœ€åsequence_lengthæ¡æ•°æ®
            latest_data = self.data.tail(sequence_length)

            # ç¡®ä¿æœ‰scaler
            if self.current_scaler is None:
                print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°scalerï¼Œé‡æ–°åˆ›å»º...")
                temp_dataset = EnhancedFC3DDataset(latest_data, sequence_length=sequence_length,
                                                   feature_columns=self.feature_columns, fit_scaler=True)
                self.current_scaler = temp_dataset.get_scaler()

            # åˆ›å»ºé¢„æµ‹æ•°æ®é›†
            dataset = EnhancedFC3DDataset(latest_data, sequence_length=sequence_length,
                                          feature_columns=self.feature_columns, fit_scaler=False,
                                          external_scaler=self.current_scaler,
                                          for_prediction=True)

            # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦ä¸ºç©º
            if len(dataset) == 0:
                print("âŒ æ•°æ®ä¸è¶³ï¼Œæ— æ³•åˆ›å»ºé¢„æµ‹åºåˆ—")
                return

            # è·å–é¢„æµ‹æ•°æ®
            features, _, _, _ = dataset[0]
            features = features.unsqueeze(0)

            # è®¡ç®—ä¸‹ä¸€æœŸæœŸå·
            next_period = self.current_period + 1

            print(f"\nğŸ¯ å¢å¼ºæ¨¡å‹é¢„æµ‹ä¸‹ä¸€æœŸå·ç  (æœŸå·: {next_period})")
            print("=" * 50)

            predictions = {}
            for model_type in trained_models:
                model_info = self.models[model_type]
                model = model_info['model']
                model.eval()

                with torch.no_grad():
                    bai_top6, shi_top6, ge_top6 = model.predict_top6(features)

                predictions[model_type] = {
                    'ç™¾ä½': bai_top6,
                    'åä½': shi_top6,
                    'ä¸ªä½': ge_top6
                }

                # æ˜¾ç¤ºæ¨¡å‹ç±»å‹å’Œæ˜¯å¦ä½¿ç”¨ä¼˜åŒ–å‚æ•°
                model_desc = model_type.upper()
                if model_info.get('optimized_params'):
                    model_desc += " (ä¼˜åŒ–å‚æ•°)"

                print(f"\nğŸ“Š {model_desc} å¢å¼ºæ¨¡å‹é¢„æµ‹:")
                print(f"   ç™¾ä½å€™é€‰: {bai_top6.tolist()}")
                print(f"   åä½å€™é€‰: {shi_top6.tolist()}")
                print(f"   ä¸ªä½å€™é€‰: {ge_top6.tolist()}")

                # æ˜¾ç¤ºæ¨èç»„åˆ
                print("   ğŸ² æ¨èç»„åˆ:")
                for i in range(min(6, len(bai_top6))):
                    combo = f"{bai_top6[i]}{shi_top6[i]}{ge_top6[i]}"
                    print(f"      {combo}")

            # ç»¼åˆæ¨è
            print(f"\nğŸŒŸ ç»¼åˆæ¨èå·ç :")
            self._generate_comprehensive_recommendation(predictions)

            logging.info("å¢å¼ºæ¨¡å‹é¢„æµ‹å®Œæˆ")

        except Exception as e:
            error_msg = f"å¢å¼ºæ¨¡å‹é¢„æµ‹å¤±è´¥: {e}"
            print(f"âŒ {error_msg}")
            logging.error(error_msg)
            import traceback
            traceback.print_exc()

    def _generate_comprehensive_recommendation(self, predictions: Dict):
        """ç”Ÿæˆç»¼åˆæ¨èå·ç """
        all_bai = []
        all_shi = []
        all_ge = []

        for model_pred in predictions.values():
            all_bai.extend(model_pred['ç™¾ä½'])
            all_shi.extend(model_pred['åä½'])
            all_ge.extend(model_pred['ä¸ªä½'])

        # ç»Ÿè®¡é¢‘ç‡
        bai_freq = pd.Series(all_bai).value_counts()
        shi_freq = pd.Series(all_shi).value_counts()
        ge_freq = pd.Series(all_ge).value_counts()

        print("   ç™¾ä½é«˜é¢‘: ", bai_freq.head(3).index.tolist())
        print("   åä½é«˜é¢‘: ", shi_freq.head(3).index.tolist())
        print("   ä¸ªä½é«˜é¢‘: ", ge_freq.head(3).index.tolist())

        # æ¨èç»„åˆ
        top_bai = bai_freq.index[0] if len(bai_freq) > 0 else 0
        top_shi = shi_freq.index[0] if len(shi_freq) > 0 else 0
        top_ge = ge_freq.index[0] if len(ge_freq) > 0 else 0

        print(f"   ğŸ’« æœ€ä¼˜æ¨è: {top_bai}{top_shi}{top_ge}")

        # æ˜¾ç¤ºå…¶ä»–æ¨èç»„åˆ
        print("   ğŸ¯ å…¶ä»–æ¨è:")
        for i in range(min(3, len(bai_freq), len(shi_freq), len(ge_freq))):
            bai = bai_freq.index[i] if i < len(bai_freq) else bai_freq.index[0]
            shi = shi_freq.index[i] if i < len(shi_freq) else shi_freq.index[0]
            ge = ge_freq.index[i] if i < len(ge_freq) else ge_freq.index[0]
            print(f"      {bai}{shi}{ge}")

    def calculate_position_accuracy(self, model, data_loader):
        """è®¡ç®—æ¯ä¸ªä½ç½®çš„å‡†ç¡®ç‡å’Œä¸‰ä¸ªä½ç½®åŒæ—¶å‘½ä¸­ç‡"""
        model.eval()
        device = next(model.parameters()).device

        bai_correct = 0
        shi_correct = 0
        ge_correct = 0
        all_correct = 0
        total_samples = 0

        with torch.no_grad():
            with ProgressBar(len(data_loader), desc="Calculating Accuracy") as pbar:
                for data, bai_target, shi_target, ge_target in data_loader:
                    data = data.to(device)
                    bai_target = bai_target.to(device).squeeze()
                    shi_target = shi_target.to(device).squeeze()
                    ge_target = ge_target.to(device).squeeze()

                    bai_probs, shi_probs, ge_probs = model(data)

                    # è·å–é¢„æµ‹ç»“æœ
                    _, bai_pred = torch.max(bai_probs, 1)
                    _, shi_pred = torch.max(shi_probs, 1)
                    _, ge_pred = torch.max(ge_probs, 1)

                    # è®¡ç®—æ¯ä¸ªä½ç½®çš„æ­£ç¡®æ•°
                    bai_correct += (bai_pred == bai_target).sum().item()
                    shi_correct += (shi_pred == shi_target).sum().item()
                    ge_correct += (ge_pred == ge_target).sum().item()

                    # è®¡ç®—ä¸‰ä¸ªä½ç½®åŒæ—¶æ­£ç¡®çš„æ•°é‡
                    all_correct += ((bai_pred == bai_target) &
                                    (shi_pred == shi_target) &
                                    (ge_pred == ge_target)).sum().item()

                    total_samples += bai_target.size(0)
                    pbar.update(1)

        return {
            'bai_accuracy': bai_correct / total_samples if total_samples > 0 else 0,
            'shi_accuracy': shi_correct / total_samples if total_samples > 0 else 0,
            'ge_accuracy': ge_correct / total_samples if total_samples > 0 else 0,
            'all_accuracy': all_correct / total_samples if total_samples > 0 else 0,
            'total_samples': total_samples
        }

    def run_backtest(self):
        """è¿è¡Œå¢å¼ºæ¨¡å‹å›æµ‹"""
        if not self.models:
            print("âŒ è¯·å…ˆè®­ç»ƒæˆ–åŠ è½½å¢å¼ºæ¨¡å‹")
            return

        print("\nğŸ“Š å¼€å§‹å¢å¼ºæ¨¡å‹å›æµ‹...")
        logging.info("å¼€å§‹å¢å¼ºæ¨¡å‹å›æµ‹")

        # ä½¿ç”¨å20%æ•°æ®ä½œä¸ºæµ‹è¯•é›†
        test_data = self.data.iloc[int((1 - EnhancedConstants.VALIDATION_SPLIT) * len(self.data)):]
        if len(test_data) < EnhancedConstants.DEFAULT_SEQUENCE_LENGTH + 1:
            print("âŒ æµ‹è¯•æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œå›æµ‹")
            return

        test_dataset = EnhancedFC3DDataset(test_data, feature_columns=self.feature_columns, fit_scaler=False,
                                           external_scaler=self.current_scaler)
        test_loader = DataLoader(test_dataset, batch_size=EnhancedConstants.BATCH_SIZE, shuffle=False, num_workers=0)

        results = {}

        trained_models = [name for name, info in self.models.items()
                          if info.get('is_trained', False)]

        if not trained_models:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„è®­ç»ƒå¥½çš„å¢å¼ºæ¨¡å‹")
            return

        for model_type in trained_models:
            print(f"\nğŸ” æµ‹è¯• {model_type} å¢å¼ºæ¨¡å‹...")
            model = self.models[model_type]['model']

            accuracy_info = self.calculate_position_accuracy(model, test_loader)
            results[model_type] = accuracy_info

            # æ˜¾ç¤ºæ¨¡å‹ç±»å‹å’Œæ˜¯å¦ä½¿ç”¨ä¼˜åŒ–å‚æ•°
            model_desc = model_type.upper()
            if self.models[model_type].get('optimized_params'):
                model_desc += " (ä¼˜åŒ–å‚æ•°)"

            print(f"   âœ… {model_desc} å›æµ‹ç»“æœ:")
            print(f"      ç™¾ä½å‡†ç¡®ç‡: {accuracy_info['bai_accuracy']:.4f}")
            print(f"      åä½å‡†ç¡®ç‡: {accuracy_info['shi_accuracy']:.4f}")
            print(f"      ä¸ªä½å‡†ç¡®ç‡: {accuracy_info['ge_accuracy']:.4f}")
            print(f"      ä¸‰ä½ç½®åŒæ—¶å‘½ä¸­ç‡: {accuracy_info['all_accuracy']:.4f}")
            print(f"      æµ‹è¯•æ ·æœ¬æ•°: {accuracy_info['total_samples']}")

        # æ˜¾ç¤ºè¯¦ç»†å›æµ‹æ€»ç»“
        self._display_backtest_summary(results)

        logging.info("å¢å¼ºæ¨¡å‹å›æµ‹å®Œæˆ")
        return results

    def _display_backtest_summary(self, results: Dict):
        """æ˜¾ç¤ºå›æµ‹è¯¦ç»†æ€»ç»“"""
        print(f"\n{'=' * 80}")
        print("ğŸ¯ å¢å¼ºæ¨¡å‹å›æµ‹è¯¦ç»†æ€»ç»“")
        print(f"{'=' * 80}")

        # åˆ›å»ºæ€»ç»“è¡¨æ ¼
        summary_data = []
        for model_type, acc_info in results.items():
            model_desc = model_type.upper()
            if self.models[model_type].get('optimized_params'):
                model_desc += " (ä¼˜åŒ–)"

            summary_data.append({
                'æ¨¡å‹': model_desc,
                'ç™¾ä½å‡†ç¡®ç‡': f"{acc_info['bai_accuracy']:.4f}",
                'åä½å‡†ç¡®ç‡': f"{acc_info['shi_accuracy']:.4f}",
                'ä¸ªä½å‡†ç¡®ç‡': f"{acc_info['ge_accuracy']:.4f}",
                'ä¸‰ä½ç½®åŒæ—¶å‘½ä¸­ç‡': f"{acc_info['all_accuracy']:.4f}",
                'æµ‹è¯•æ ·æœ¬': acc_info['total_samples']
            })

        # æ‰“å°è¡¨æ ¼
        for data in summary_data:
            print(f"ğŸ“Š {data['æ¨¡å‹']}:")
            print(f"   ç™¾ä½: {data['ç™¾ä½å‡†ç¡®ç‡']} | åä½: {data['åä½å‡†ç¡®ç‡']} | "
                  f"ä¸ªä½: {data['ä¸ªä½å‡†ç¡®ç‡']} | ä¸‰ä½ç½®åŒæ—¶: {data['ä¸‰ä½ç½®åŒæ—¶å‘½ä¸­ç‡']}")

        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
        if results:
            best_model = max(results.items(), key=lambda x: x[1]['all_accuracy'])
            model_desc = best_model[0].upper()
            if self.models[best_model[0]].get('optimized_params'):
                model_desc += " (ä¼˜åŒ–)"

            print(f"\nğŸ† æœ€ä½³è¡¨ç°å¢å¼ºæ¨¡å‹: {model_desc}")
            print(f"   ä¸‰ä½ç½®åŒæ—¶å‘½ä¸­ç‡: {best_model[1]['all_accuracy']:.4f}")
        print(f"{'=' * 80}")

    def show_model_info(self):
        """æ˜¾ç¤ºå¢å¼ºæ¨¡å‹ä¿¡æ¯"""
        if not self.models:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„å¢å¼ºæ¨¡å‹")
            return

        trained_models = [name for name, info in self.models.items()
                          if info.get('is_trained', False)]

        if not trained_models:
            print("âŒ æ²¡æœ‰è®­ç»ƒå¥½çš„å¢å¼ºæ¨¡å‹")
            return

        print("\nğŸ“ˆ å¢å¼ºæ¨¡å‹è¯¦ç»†ä¿¡æ¯:")
        print("=" * 60)

        for model_type in trained_models:
            model_info = self.models[model_type]
            model_desc = model_type.upper()
            if model_info.get('optimized_params'):
                model_desc += " (ä¼˜åŒ–å‚æ•°)"

            print(f"\nğŸ”§ {model_desc} å¢å¼ºæ¨¡å‹:")
            if 'info' in model_info:
                info = model_info['info']
                print(f"   æœ€ä½³éªŒè¯æŸå¤±: {info.get('best_val_loss', 'N/A'):.6f}")
                print(f"   æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {info.get('best_val_acc', 'N/A'):.4f}")
                print(f"   è®­ç»ƒè½®æ¬¡: {info.get('final_epoch', 'N/A')}")
                print(f"   è®­ç»ƒæ—¶é—´: {info.get('training_time', 'N/A'):.2f}ç§’")

            # æ˜¾ç¤ºæ¨¡å‹å‚æ•°æ•°é‡
            model = model_info['model']
            total_params = sum(p.numel() for p in model.parameters())
            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            print(f"   æ€»å‚æ•°é‡: {total_params:,}")
            print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
            print(f"   è®­ç»ƒçŠ¶æ€: {'âœ… å·²è®­ç»ƒ' if model_info.get('is_trained', False) else 'âŒ æœªè®­ç»ƒ'}")

            # æ˜¾ç¤ºä¼˜åŒ–å‚æ•°ä¿¡æ¯
            if model_info.get('optimized_params'):
                print(f"   ä¼˜åŒ–å‚æ•°: {model_info['optimized_params']}")

    def show_optimization_results(self):
        """æ˜¾ç¤ºä¼˜åŒ–ç»“æœ"""
        if not self.optimization_results:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä¼˜åŒ–ç»“æœ")
            return

        print(f"\n{'=' * 80}")
        print("ğŸ“Š å¢å¼ºè¶…å‚æ•°ä¼˜åŒ–ç»“æœæ€»ç»“")
        print(f"{'=' * 80}")

        for model_type, results in self.optimization_results.items():
            best_params = results.get('best_params', {})
            best_score = results.get('best_score', 0)

            print(f"\nğŸ”§ {model_type.upper()}:")
            print(f"   æœ€ä½³å‡†ç¡®ç‡: {best_score:.4f}")
            print(f"   æœ€ä¼˜å‚æ•°:")
            for param, value in best_params.items():
                print(f"     {param}: {value}")

        print(f"{'=' * 80}")

    def show_performance_history(self):
        """æ˜¾ç¤ºæ€§èƒ½å†å²"""
        print(f"\n{'=' * 80}")
        print("ğŸ“ˆ å¢å¼ºæ¨¡å‹æ€§èƒ½å†å²")
        print(f"{'=' * 80}")

        for model_type in self.models.keys():
            summary = self.performance_tracker.get_performance_summary(model_type)
            if summary:
                model_desc = model_type.upper()
                if self.models[model_type].get('optimized_params'):
                    model_desc += " (ä¼˜åŒ–)"

                print(f"\nğŸ” {model_desc}:")
                print(f"   æ€»é¢„æµ‹æ¬¡æ•°: {summary['total_predictions']}")
                print(f"   ç™¾ä½å‘½ä¸­ç‡: {summary['accuracy_rates']['bai']:.4f}")
                print(f"   åä½å‘½ä¸­ç‡: {summary['accuracy_rates']['shi']:.4f}")
                print(f"   ä¸ªä½å‘½ä¸­ç‡: {summary['accuracy_rates']['ge']:.4f}")
                print(f"   ä¸‰ä½ç½®åŒæ—¶å‘½ä¸­ç‡: {summary['accuracy_rates']['all']:.4f}")

        print(f"{'=' * 80}")

    def _run_hyperparameter_optimization_menu(self):
        """å¢å¼ºè¶…å‚æ•°ä¼˜åŒ–å­èœå•"""
        opt_menu = """
âš™ï¸  å¢å¼ºè¶…å‚æ•°ä¼˜åŒ–èœå•

1. ğŸ”¬ ä¼˜åŒ–å¢å¼ºæ—¶åºæ··åˆæ¨¡å‹
2. ğŸ§  ä¼˜åŒ–å¢å¼ºLSTMæ¨¡å‹  
3. ğŸ”— ä¼˜åŒ–å¢å¼ºTransformeræ¨¡å‹
4. ğŸ“Š æŸ¥çœ‹å¢å¼ºä¼˜åŒ–ç»“æœ
5. â†©ï¸  è¿”å›ä¸»èœå•

è¯·é€‰æ‹© (1-5): """

        while True:
            choice = input(opt_menu).strip()

            if choice == '1':
                self.optimize_hyperparameters('temporal_moe', n_trials=20, timeout=1800)
            elif choice == '2':
                self.optimize_hyperparameters('attention_lstm', n_trials=20, timeout=1800)
            elif choice == '3':
                self.optimize_hyperparameters('transformer', n_trials=20, timeout=1800)
            elif choice == '4':
                self.show_optimization_results()
            elif choice == '5':
                break
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©")

            input("\næŒ‰Enteré”®ç»§ç»­...")

    def run_interactive_menu(self):
        """è¿è¡Œäº¤äº’å¼èœå•"""
        menu = """
ğŸ° FC3Dæ™ºèƒ½äº¤äº’å¼é¢„æµ‹ç³»ç»Ÿ - å¢å¼ºä¼˜åŒ–ç‰ˆ ğŸ°

1. ğŸ“‚ åŠ è½½æ•°æ®
2. ğŸš€ è®­ç»ƒå¢å¼ºæ—¶åºæ··åˆæ¨¡å‹  
3. ğŸ§  è®­ç»ƒå¢å¼ºLSTMæ¨¡å‹
4. ğŸ”— è®­ç»ƒå¢å¼ºTransformeræ¨¡å‹
5. ğŸ“¥ åŠ è½½å·²æœ‰å¢å¼ºæ¨¡å‹
6. ğŸ”® é¢„æµ‹ä¸‹ä¸€æœŸå·ç 
7. ğŸ“Š è¿è¡Œå„ä¸ªå¢å¼ºæ¨¡å‹å›æµ‹
8. ğŸ“ˆ æŸ¥çœ‹å¢å¼ºæ¨¡å‹ä¿¡æ¯
9. âš™ï¸  å¢å¼ºè¶…å‚æ•°ä¼˜åŒ–ï¼ˆæå‡æ€§èƒ½ï¼‰
10. ğŸ“‹ æŸ¥çœ‹æ€§èƒ½å†å²
11. ğŸ’¾ ä¿å­˜ç³»ç»ŸçŠ¶æ€
12. ğŸšª é€€å‡ºç³»ç»Ÿ

è¯·é€‰æ‹©æ“ä½œ (1-12): """

        while True:
            try:
                choice = input(menu).strip()

                if choice == '1':
                    file_path = "/Users/uajxjd/Desktop/UAFC3D.csv"
                    self.load_data(file_path)

                elif choice == '2':
                    self.train_model('temporal_moe')

                elif choice == '3':
                    self.train_model('attention_lstm')

                elif choice == '4':
                    self.train_model('transformer')

                elif choice == '5':
                    loaded_count = self.load_existing_models()
                    if loaded_count > 0:
                        print(f"âœ… æˆåŠŸåŠ è½½ {loaded_count} ä¸ªå¢å¼ºæ¨¡å‹")
                    else:
                        print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯åŠ è½½çš„å¢å¼ºæ¨¡å‹æ–‡ä»¶")

                elif choice == '6':
                    self.predict_next_period()

                elif choice == '7':
                    self.run_backtest()

                elif choice == '8':
                    self.show_model_info()

                elif choice == '9':
                    self._run_hyperparameter_optimization_menu()

                elif choice == '10':
                    self.show_performance_history()

                elif choice == '11':
                    if ConfigManager.save_config(self):
                        print("âœ… ç³»ç»ŸçŠ¶æ€å·²ä¿å­˜")
                    else:
                        print("âŒ ç³»ç»ŸçŠ¶æ€ä¿å­˜å¤±è´¥")

                elif choice == '12':
                    # ä¿å­˜æ€§èƒ½æ•°æ®å’Œé…ç½®
                    self.performance_tracker.save_performance_data()
                    ConfigManager.save_config(self)
                    print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨å¢å¼ºç‰ˆFC3Dé¢„æµ‹ç³»ç»Ÿ!")
                    logging.info("å¢å¼ºç‰ˆç³»ç»Ÿæ­£å¸¸é€€å‡º")
                    break

                else:
                    print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")

                input("\næŒ‰Enteré”®ç»§ç»­...")

            except KeyboardInterrupt:
                print("\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œé€€å‡ºç³»ç»Ÿ")
                self.performance_tracker.save_performance_data()
                ConfigManager.save_config(self)
                logging.info("å¢å¼ºç‰ˆç³»ç»Ÿè¢«ç”¨æˆ·ä¸­æ–­")
                break
            except Exception as e:
                error_msg = f"å‘ç”Ÿé”™è¯¯: {e}"
                print(f"âŒ {error_msg}")
                logging.error(error_msg)
                import traceback
                traceback.print_exc()


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("           FC3Dæ™ºèƒ½äº¤äº’å¼é¢„æµ‹ç³»ç»Ÿ - å¢å¼ºä¼˜åŒ–ç‰ˆ")
    print("       åŸºäºå…ˆè¿›AIæ¶æ„å’Œå¢å¼ºè®­ç»ƒç­–ç•¥å¼€å‘")
    print("=" * 80)

    # æ£€æŸ¥ä¾èµ–
    try:
        import tqdm
        import sklearn
        import optuna
        print("âœ… æ‰€æœ‰ä¾èµ–å·²å°±ç»ª")
    except ImportError as e:
        print(f"âŒ ç¼ºå°‘ä¾èµ–: {e}")
        print("è¯·è¿è¡Œ: pip install tqdm matplotlib scikit-learn optuna")
        return

    predictor = EnhancedFC3DPredictor()
    predictor.run_interactive_menu()


if __name__ == "__main__":
    main()